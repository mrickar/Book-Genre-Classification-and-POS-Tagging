{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rm8WIg_ioGZR"
      },
      "source": [
        "#Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlRuqH3QB_23",
        "outputId": "e2b2a0b2-fce2-4293-e268-69d34dd926ce"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "import sklearn\n",
        "import string\n",
        "import pickle\t# this is for saving and loading your trained classifiers.\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from nltk.classify.scikitlearn import SklearnClassifier\n",
        "from nltk import NaiveBayesClassifier\n",
        "\n",
        "#Preprocessing\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk import FreqDist\n",
        "from nltk.lm import Vocabulary\n",
        "from nltk.util import ngrams\n",
        "\n",
        "#Evaluating\n",
        "from nltk.metrics import ConfusionMatrix\n",
        "\n",
        "\n",
        "import random\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eFWnvg-oVzO"
      },
      "source": [
        "#Constant values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BZq2_OoYj20l"
      },
      "outputs": [],
      "source": [
        "genres=[\"philosophy\", \"science-fiction\", \"romance\", \"horror\", \"science\", \"religion\", \"mystery\", \"sports\"]\n",
        "# If txt files are not in same folder with notebook this should be added here\n",
        "train_path = \"\" \n",
        "dev_path = \"\"\n",
        "test_path = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA6SZJUOoZE0"
      },
      "source": [
        "#File operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bnjjChMSCE7s"
      },
      "outputs": [],
      "source": [
        "def read_file_lines(file_path):\n",
        "    file = open(file_path, 'r',encoding='utf-8')\n",
        "    lines = file.read().splitlines()\n",
        "    file.close()\n",
        "    return lines\n",
        "\n",
        "def save_classifier(classifier, filename):\t#filename should end with .pickle and type(filename)=string\n",
        "\twith open(filename, \"wb\") as f:\n",
        "\t\tpickle.dump(classifier, f)\n",
        "\treturn\n",
        "def load_classifier(filename):\t#filename should end with .pickle and type(filename)=string\n",
        "\tclassifier_file = open(filename, \"rb\")\n",
        "\tclassifier = pickle.load(classifier_file)\n",
        "\tclassifier_file.close()\n",
        "\treturn classifier\n",
        "def write_list(lst,name):\n",
        "\twith open(name, 'w') as file:\n",
        "\t\tfor item in lst:\n",
        "\t\t\tfile.write(f\"{item}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYakQbG-oeIy"
      },
      "source": [
        "#Preproccess Operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "w8psD9R9CGnF"
      },
      "outputs": [],
      "source": [
        "def preprocess(texts,class_name):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    processed = []\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    total_book_count = len(texts)//2\n",
        "    for i in range(total_book_count):\n",
        "        header = texts[2*i]\n",
        "        text = texts[2*i+1]\n",
        "\n",
        "        processed_text = text.lower()\n",
        "        words_text = word_tokenize(processed_text)\n",
        "        processed_header = header.lower()\n",
        "        words_header = word_tokenize(processed_header)\n",
        "\n",
        "        words_text =words_header + words_text\n",
        "        processed_words_text = [lemmatizer.lemmatize(word) for word in words_text if word.isalnum() and (word not in stop_words) and len(word) > 1]\n",
        "        processed_words_header = [\"HEAD_\" + lemmatizer.lemmatize(word) for word in words_header  if word.isalnum() and (word not in stop_words) and len(word) > 1]\n",
        "\n",
        "        # porter_stemmer = PorterStemmer()\n",
        "        # processed_words_text = [porter_stemmer.stem(word) for word in words_text if word.isalnum() and (word not in stop_words) and len(word) > 1]\n",
        "        # processed_words_header = [\"HEAD_\" + porter_stemmer.stem(word) for word in words_header  if word.isalnum() and (word not in stop_words) and len(word) > 1]\n",
        "\n",
        "        processed_text = ' '.join(processed_words_text)\n",
        "        processed_header = ' '.join(processed_words_header)\n",
        "\n",
        "        book = processed_header + \" \" + processed_text\n",
        "        processed.append((book ,class_name))\n",
        "    return processed\n",
        "\n",
        "def create_megadoc(dataset_type,path=\"\"): #dataset_type type as train, test, dev\n",
        "    file_extension = \"_\"+dataset_type + \".txt\"\n",
        "    megadoc = []\n",
        "    for genre in genres:\n",
        "        file_name = genre + file_extension\n",
        "        txt = read_file_lines(path + file_name)\n",
        "        megadoc += preprocess(texts=txt,class_name=genre)\n",
        "    return megadoc\n",
        "\n",
        "def create_vocabulary(mega_doc):\n",
        "    all_words = [word for text_class_pair in mega_doc for word in text_class_pair[0].split()]\n",
        "    vocab = Vocabulary(all_words, unk_cutoff=10)\n",
        "    return vocab\n",
        "def split_mega_doc(megadoc):\n",
        "    genre_texts={genre:[] for genre in genres}\n",
        "    for text, label in megadoc:\n",
        "        genre_texts[label] += [text]\n",
        "    return genre_texts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfbBX50No8TL"
      },
      "source": [
        "Creating mega documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGBCeVYlWNtE",
        "outputId": "fd31caa3-8fdd-468a-b7c6-4e3a5b2a4e2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_megadoc size = 6536\n",
            "dev_megadoc size = 933\n",
            "test_megadoc size = 1865\n"
          ]
        }
      ],
      "source": [
        "train_megadoc = create_megadoc(dataset_type=\"train\")\n",
        "test_megadoc = create_megadoc(dataset_type = \"test\")\n",
        "dev_megadoc = create_megadoc(dataset_type = \"dev\")\n",
        "print(f\"train_megadoc size = {len(train_megadoc)}\")\n",
        "print(f\"dev_megadoc size = {len(dev_megadoc)}\")\n",
        "print(f\"test_megadoc size = {len(test_megadoc)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyHfyzwAxMlo"
      },
      "source": [
        "## Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1waRGgDcxPMa"
      },
      "outputs": [],
      "source": [
        "def extract_features(megadoc,features):\n",
        "    features_labels = []\n",
        "    for text,label in megadoc:\n",
        "        cur_features = {}\n",
        "        for feature in features:\n",
        "            cur_features[feature] = feature in text\n",
        "        features_labels.append((cur_features,label))\n",
        "    return features_labels\n",
        "\n",
        "def get_ngrams(text:str,n):\n",
        "    tokens = ngrams(word_tokenize(text),n)\n",
        "    tokens = [\" \".join(ngram) for ngram in tokens]\n",
        "    return tokens\n",
        "\n",
        "def most_frequent_ngrams(texts:list,count,ngram_n):\n",
        "    whole_texts = \" \".join(texts)\n",
        "    tokens = get_ngrams(whole_texts,ngram_n)\n",
        "    freqs = FreqDist(tokens).most_common(count)\n",
        "    # print(freqs)\n",
        "    words = [word for word,freq in freqs]\n",
        "    return words\n",
        "\n",
        "def most_freq_ngram_each_label(megadoc,count,ngram_n):\n",
        "    splitted_megadoc = split_mega_doc(train_megadoc)\n",
        "    freq_words = []\n",
        "    for label in splitted_megadoc.keys():\n",
        "        freq_words.extend(most_frequent_ngrams(splitted_megadoc[label],count[label],ngram_n))\n",
        "\n",
        "    return list(set(freq_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVs9tINAS0iw"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "uEDWFvRkCH-v"
      },
      "outputs": [],
      "source": [
        "def train(classifier, training_set):\n",
        "    return classifier.train(training_set)\n",
        "\n",
        "\n",
        "def test(classifier, test_set):\n",
        "    actual_labels = [label for feature,label in test_set]\n",
        "    feature_list = [feature for feature,label in test_set]\n",
        "    predictions = classifier.classify_many(feature_list)\n",
        "    confusion_matrix = ConfusionMatrix(actual_labels,predictions)\n",
        "    print(confusion_matrix.pretty_format())\n",
        "    print(confusion_matrix.evaluate())\n",
        "\n",
        "    accuracy_score = nltk.scores.accuracy(actual_labels, predictions)\n",
        "    print(f\"Accuracy: {accuracy_score}\")\n",
        "    return confusion_matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgPlezZ9OwCA"
      },
      "source": [
        "# Analyze Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHA7ix2LOvpn",
        "outputId": "fff5fe44-6773-42d9-c5ab-018c52c9ac74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "philosophy\n",
            "798\n",
            "word count: 67852\n",
            "---------\n",
            "science-fiction\n",
            "840\n",
            "word count: 66522\n",
            "---------\n",
            "romance\n",
            "798\n",
            "word count: 70148\n",
            "---------\n",
            "horror\n",
            "832\n",
            "word count: 63491\n",
            "---------\n",
            "science\n",
            "805\n",
            "word count: 91706\n",
            "---------\n",
            "religion\n",
            "805\n",
            "word count: 80866\n",
            "---------\n",
            "mystery\n",
            "840\n",
            "word count: 72274\n",
            "---------\n",
            "sports\n",
            "818\n",
            "word count: 82386\n",
            "---------\n"
          ]
        }
      ],
      "source": [
        "splitted_megadoc = split_mega_doc(train_megadoc)\n",
        "for key in splitted_megadoc.keys():\n",
        "    print(key)\n",
        "    book_lst = splitted_megadoc[key]\n",
        "    print(len(book_lst))\n",
        "    text = \" \".join(book_lst)\n",
        "    words = word_tokenize(text)\n",
        "    print(f\"word count: {len(words)}\")\n",
        "    print(\"---------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wphpLILphfs"
      },
      "source": [
        "#Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2bzePqD8eb1"
      },
      "source": [
        "## Unigrams\n",
        "In this part I am trying to use most frequent unigrams in each class as feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XME9PDp1-aGC"
      },
      "source": [
        "I am taking most frequent 500 words in each class since there are many same words in each class. The feature set has size nearly 1500."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gzhxv6rP4FTT",
        "outputId": "5d00eca3-7bc8-46c6-a903-c71aee78c8a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1429\n",
            "['destroyed', 'want', 'robot', 'subject', 'best', 'bone', 'root', 'prayer', 'church', 'design']\n"
          ]
        }
      ],
      "source": [
        "counts_per_label={genre:500 for genre in genres}\n",
        "unigrams = most_freq_ngram_each_label(train_megadoc,counts_per_label,1)\n",
        "print(len(unigrams))\n",
        "print(unigrams[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gXL1p07r4Y3g"
      },
      "outputs": [],
      "source": [
        "# training_set = extract_features(train_megadoc,unigrams)\n",
        "# dev_set = extract_features(dev_megadoc,unigrams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fahn7g6I8xmv"
      },
      "outputs": [],
      "source": [
        "# naive_classifier_unig = train(NaiveBayesClassifier,training_set)\n",
        "# save_classifier(naive_classifier_unig,\"naive_bayes_unigram_freqs.pickle\")\n",
        "# naive_bayes_unig_cm = test(naive_classifier_unig,dev_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vo5SoirI85AD"
      },
      "outputs": [],
      "source": [
        "# naive_classifier_unig.show_most_informative_features(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HPQO6RnfXlA"
      },
      "source": [
        "## Bigrams\n",
        "In this part I am trying to use most frequent bigrams in each class as feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bf5Qghb0gKEV",
        "outputId": "908449d1-cfd0-4a4a-bcb4-2f39e8d6b3b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "851\n",
            "['story collection', 'history christianity', 'take one']\n"
          ]
        }
      ],
      "source": [
        "counts_per_label={genre:150 for genre in genres}\n",
        "bigrams = most_freq_ngram_each_label(train_megadoc,counts_per_label,2)\n",
        "print(len(bigrams))\n",
        "print(bigrams[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "626zpTUZfW6V"
      },
      "outputs": [],
      "source": [
        "# training_set = extract_features(train_megadoc,bigrams)\n",
        "# dev_set = extract_features(dev_megadoc,bigrams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8MAY-hMifdxJ"
      },
      "outputs": [],
      "source": [
        "# naive_classifier_bigr = train(NaiveBayesClassifier,training_set)\n",
        "# save_classifier(naive_classifier_bigr,\"naive_bayes_bigram_freqs.pickle\")\n",
        "# naive_bayes_bigr_cm = test(naive_classifier_bigr,dev_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Dieg0EuusX4F"
      },
      "outputs": [],
      "source": [
        "# naive_classifier_bigr.show_most_informative_features(300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZJfLx3_BPBX"
      },
      "source": [
        "## Trigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "764U9IP6Bsr0"
      },
      "outputs": [],
      "source": [
        "# counts_per_label={genre:80 for genre in genres}\n",
        "# # counts_per_label[\"science-fiction\"] = 40\n",
        "# trigrams = most_freq_ngram_each_label(train_megadoc,counts_per_label,3)\n",
        "# print(len(trigrams))\n",
        "# # print(trigrams[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "VdamsMBCB1p6"
      },
      "outputs": [],
      "source": [
        "# training_set = extract_features(train_megadoc,trigrams)\n",
        "# dev_set = extract_features(dev_megadoc,trigrams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "UjvMCjSEB4wl"
      },
      "outputs": [],
      "source": [
        "# naive_classifier_trig = train(NaiveBayesClassifier,training_set)\n",
        "# naive_bayes_trig_cm = test(naive_classifier_trig,dev_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "L0ZRab4ufK5X"
      },
      "outputs": [],
      "source": [
        "# naive_classifier_trig.show_most_informative_features(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "f53Eav5CCA_w"
      },
      "outputs": [],
      "source": [
        "# # naive_classifier_trig.show_most_informative_features(100)\n",
        "# trigram_most_inf = [trigram for trigram,correct in naive_classifier_trig.most_informative_features(100) if correct]\n",
        "# print(trigram_most_inf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C4duJMFAkyh"
      },
      "source": [
        "## Combining n-grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before running the cell below cells assigning the unigram and bigram should be run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27uuCfDuAxSA",
        "outputId": "f0689d3e-4a15-4d5c-ed84-4a99c24b701e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2380\n"
          ]
        }
      ],
      "source": [
        "trigram_most_inf = ['york time bestselling', 'time bestselling author', 'major league baseball', 'edgar allan poe', 'high school football', 'york time bestseller', 'world war ii', 'usa today bestselling', 'two thousand year', 'novel new york', 'science fiction novel', 'new york city', 'time book review', 'york time book', 'change way see', 'science fiction adventure', 'librarian note alternate', 'time literary supplement', 'note alternate cover', 'make u human', 'tour de force', 'world fantasy award', 'time usa today', 'york time usa', 'orson scott card', 'alternate cover edition', 'today bestselling author', 'must work together', 'los angeles time', 'million year ago', 'alternate cover isbn', 'ralph waldo emerson', 'second world war', 'cover edition found', 'john stuart mill', 'one new york', 'nature space time', 'dream come true', 'cover edition isbn', 'professional hockey player', 'one science fiction', 'new york time', 'find falling love', 'installment new york', 'since first publication', 'thing need know', 'national book award', 'shed new light', 'waitress sookie stackhouse', 'way see world', 'school football team', 'author new york', 'time bestselling novel', 'twenty year ago', 'one give full', 'five year ago', 'make matter worse', 'lieutenant eve dallas', 'essential reading anyone', 'theory natural selection', 'thousand year ago', 'arthur conan doyle', 'edition isbn found', 'new world order', 'everything thought knew', 'hit rock bottom', 'alternative cover edition', 'science fiction author', 'science fiction writer', 'stephen jay gould', 'boy next door', 'four year ago', 'brave new world', 'cover isbn found', 'get life back', 'one thing certain', 'york time author', 'one got away', 'year high school', 'romantic comedy novel', 'man woman child', 'anthropologist temperance brennan', 'ayaan hirsi ali', 'come back haunt', 'forensic anthropologist temperance', 'hundred year future', 'year passed since', 'forced question everything', 'masterpiece science fiction', 'science fiction classic', 'desert planet arrakis', 'cocktail waitress sookie', 'last thing expects', 'world around u', 'bestselling mortal instrument', 'love first sight', 'never knew existed', 'would change life', 'da vinci code', 'one twentieth century']\n",
        "ngram_features = unigrams + bigrams + trigram_most_inf\n",
        "print(len(ngram_features))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "E8Nw73XQA7tV"
      },
      "outputs": [],
      "source": [
        "training_set = extract_features(train_megadoc,ngram_features)\n",
        "dev_set = extract_features(dev_megadoc,ngram_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocChu0WsA_yk",
        "outputId": "ce84b7ee-f303-4dca-f384-ea14e124d5bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                |                    s    |\n",
            "                |                    c    |\n",
            "                |                    i    |\n",
            "                |                    e    |\n",
            "                |                    n    |\n",
            "                |        p           c    |\n",
            "                |        h           e    |\n",
            "                |        i  r        -    |\n",
            "                |     m  l  e  r  s  f    |\n",
            "                |  h  y  o  l  o  c  i  s |\n",
            "                |  o  s  s  i  m  i  c  p |\n",
            "                |  r  t  o  g  a  e  t  o |\n",
            "                |  r  e  p  i  n  n  i  r |\n",
            "                |  o  r  h  o  c  c  o  t |\n",
            "                |  r  y  y  n  e  e  n  s |\n",
            "----------------+-------------------------+\n",
            "         horror |<80>15  2  .  9  1 10  1 |\n",
            "        mystery | 13<95> .  .  9  .  2  1 |\n",
            "     philosophy |  3  2<82>14  . 11  2  . |\n",
            "       religion |  6  3 27<69> 3  3  4  . |\n",
            "        romance |  6  1  1  2<90> . 10  4 |\n",
            "        science |  5  1 11  3  .<86> 9  . |\n",
            "science-fiction | 18  5  .  1  7  1<85> 3 |\n",
            "         sports |  1  .  1  1 14  1  2<97>|\n",
            "----------------+-------------------------+\n",
            "(row = reference; col = test)\n",
            "\n",
            "            Tag | Prec.  | Recall | F-measure\n",
            "----------------+--------+--------+-----------\n",
            "         horror | 0.6061 | 0.6780 | 0.6400\n",
            "        mystery | 0.7787 | 0.7917 | 0.7851\n",
            "     philosophy | 0.6613 | 0.7193 | 0.6891\n",
            "       religion | 0.7667 | 0.6000 | 0.6732\n",
            "        romance | 0.6818 | 0.7895 | 0.7317\n",
            "        science | 0.8350 | 0.7478 | 0.7890\n",
            "science-fiction | 0.6855 | 0.7083 | 0.6967\n",
            "         sports | 0.9151 | 0.8291 | 0.8700\n",
            "\n",
            "Accuracy: 0.7331189710610932\n"
          ]
        }
      ],
      "source": [
        "naive_classifier_ngr = train(NaiveBayesClassifier,training_set)\n",
        "save_classifier(naive_classifier_ngr,\"naive_bayes_ngram_freqs.pickle\")\n",
        "naive_bayes_ngr_cm = test(naive_classifier_ngr,dev_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeegXv9dH5hN"
      },
      "source": [
        "## Naive Bayes Testing\n",
        "Testing model with test_set by using model in n-grams which is combination of unigram+bigram+trigram."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tp3HcHIuHt5V",
        "outputId": "0ba8299f-91af-4582-a970-175206198863"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                |                           s     |\n",
            "                |                           c     |\n",
            "                |                           i     |\n",
            "                |                           e     |\n",
            "                |                           n     |\n",
            "                |           p               c     |\n",
            "                |           h               e     |\n",
            "                |           i   r           -     |\n",
            "                |       m   l   e   r   s   f     |\n",
            "                |   h   y   o   l   o   c   i   s |\n",
            "                |   o   s   s   i   m   i   c   p |\n",
            "                |   r   t   o   g   a   e   t   o |\n",
            "                |   r   e   p   i   n   n   i   r |\n",
            "                |   o   r   h   o   c   c   o   t |\n",
            "                |   r   y   y   n   e   e   n   s |\n",
            "----------------+---------------------------------+\n",
            "         horror |<167> 21   3   3  17   .  23   . |\n",
            "        mystery |  29<180>  .   2  21   1   7   . |\n",
            "     philosophy |   5   1<171> 21   2  20   8   . |\n",
            "       religion |  17   3  56<126>  8   8  11   1 |\n",
            "        romance |  17   8   1   1<161>  .  21  19 |\n",
            "        science |   8   2  35   6   1<163> 13   2 |\n",
            "science-fiction |  39   8   4   .  15   4<166>  4 |\n",
            "         sports |  13   .   1   .  42   2   2<175>|\n",
            "----------------+---------------------------------+\n",
            "(row = reference; col = test)\n",
            "\n",
            "            Tag | Prec.  | Recall | F-measure\n",
            "----------------+--------+--------+-----------\n",
            "         horror | 0.5661 | 0.7137 | 0.6314\n",
            "        mystery | 0.8072 | 0.7500 | 0.7775\n",
            "     philosophy | 0.6310 | 0.7500 | 0.6854\n",
            "       religion | 0.7925 | 0.5478 | 0.6478\n",
            "        romance | 0.6030 | 0.7061 | 0.6505\n",
            "        science | 0.8232 | 0.7087 | 0.7617\n",
            "science-fiction | 0.6614 | 0.6917 | 0.6762\n",
            "         sports | 0.8706 | 0.7447 | 0.8028\n",
            "\n",
            "Accuracy: 0.7018766756032172\n"
          ]
        }
      ],
      "source": [
        "final_features = ngram_features\n",
        "test_set = extract_features(test_megadoc,ngram_features)\n",
        "naive_bayes_ngr_cm = test(naive_classifier_ngr,test_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRGmx620bYBg"
      },
      "source": [
        "Accuracy: 0.7018766756032172"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ei6eQFSPgZBg"
      },
      "source": [
        "# SVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "hcTUswmBIvHD"
      },
      "outputs": [],
      "source": [
        "training_set = extract_features(train_megadoc,final_features)\n",
        "dev_set = extract_features(dev_megadoc,final_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88nVYs3c5wzP",
        "outputId": "8259a72b-a38c-4c8d-bd07-79f52009366e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                |                    s    |\n",
            "                |                    c    |\n",
            "                |                    i    |\n",
            "                |                    e    |\n",
            "                |                    n    |\n",
            "                |        p           c    |\n",
            "                |        h           e    |\n",
            "                |        i  r        -    |\n",
            "                |     m  l  e  r  s  f    |\n",
            "                |  h  y  o  l  o  c  i  s |\n",
            "                |  o  s  s  i  m  i  c  p |\n",
            "                |  r  t  o  g  a  e  t  o |\n",
            "                |  r  e  p  i  n  n  i  r |\n",
            "                |  o  r  h  o  c  c  o  t |\n",
            "                |  r  y  y  n  e  e  n  s |\n",
            "----------------+-------------------------+\n",
            "         horror |<73>14  3  .  9  . 18  1 |\n",
            "        mystery | 18<89> .  .  9  .  4  . |\n",
            "     philosophy |  2  2<70>22  1 14  3  . |\n",
            "       religion |  1  4 19<79> 3  4  5  . |\n",
            "        romance |  6  5  2  .<84> . 10  7 |\n",
            "        science |  4  .  5  5  .<92> 8  1 |\n",
            "science-fiction | 20  6  1  2  9  1<79> 2 |\n",
            "         sports |  .  1  2  1 16  1  .<96>|\n",
            "----------------+-------------------------+\n",
            "(row = reference; col = test)\n",
            "\n",
            "            Tag | Prec.  | Recall | F-measure\n",
            "----------------+--------+--------+-----------\n",
            "         horror | 0.5887 | 0.6186 | 0.6033\n",
            "        mystery | 0.7355 | 0.7417 | 0.7386\n",
            "     philosophy | 0.6863 | 0.6140 | 0.6481\n",
            "       religion | 0.7248 | 0.6870 | 0.7054\n",
            "        romance | 0.6412 | 0.7368 | 0.6857\n",
            "        science | 0.8214 | 0.8000 | 0.8106\n",
            "science-fiction | 0.6220 | 0.6583 | 0.6397\n",
            "         sports | 0.8972 | 0.8205 | 0.8571\n",
            "\n",
            "Accuracy: 0.7095391211146839\n"
          ]
        }
      ],
      "source": [
        "SVC_classifier = train(SklearnClassifier(SVC()),training_set)\n",
        "save_classifier(SVC_classifier,\"SVC.pickle\")\n",
        "SVC_cm = test(SVC_classifier,dev_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SVC Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CocUmz5UKk-2",
        "outputId": "41629f02-93bd-481c-d6b5-1b2900eb88d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                |                           s     |\n",
            "                |                           c     |\n",
            "                |                           i     |\n",
            "                |                           e     |\n",
            "                |                           n     |\n",
            "                |           p               c     |\n",
            "                |           h               e     |\n",
            "                |           i   r           -     |\n",
            "                |       m   l   e   r   s   f     |\n",
            "                |   h   y   o   l   o   c   i   s |\n",
            "                |   o   s   s   i   m   i   c   p |\n",
            "                |   r   t   o   g   a   e   t   o |\n",
            "                |   r   e   p   i   n   n   i   r |\n",
            "                |   o   r   h   o   c   c   o   t |\n",
            "                |   r   y   y   n   e   e   n   s |\n",
            "----------------+---------------------------------+\n",
            "         horror |<163> 25   3   1  17   1  23   1 |\n",
            "        mystery |  36<169>  1   2  19   .  12   1 |\n",
            "     philosophy |   1   1<161> 31   2  25   7   . |\n",
            "       religion |  15   4  43<146>  4   8   9   1 |\n",
            "        romance |  21  11   2   .<153>  .  21  20 |\n",
            "        science |   5   3  30  10   1<169> 12   . |\n",
            "science-fiction |  40   9   4   2  14   6<161>  4 |\n",
            "         sports |   6   1   2   .  33   3   2<188>|\n",
            "----------------+---------------------------------+\n",
            "(row = reference; col = test)\n",
            "\n",
            "            Tag | Prec.  | Recall | F-measure\n",
            "----------------+--------+--------+-----------\n",
            "         horror | 0.5679 | 0.6966 | 0.6257\n",
            "        mystery | 0.7578 | 0.7042 | 0.7300\n",
            "     philosophy | 0.6545 | 0.7061 | 0.6793\n",
            "       religion | 0.7604 | 0.6348 | 0.6919\n",
            "        romance | 0.6296 | 0.6711 | 0.6497\n",
            "        science | 0.7972 | 0.7348 | 0.7647\n",
            "science-fiction | 0.6518 | 0.6708 | 0.6612\n",
            "         sports | 0.8744 | 0.8000 | 0.8356\n",
            "\n",
            "Accuracy: 0.7024128686327078\n"
          ]
        }
      ],
      "source": [
        "test_set = extract_features(test_megadoc,final_features)\n",
        "SVC_cm = test(SVC_classifier,test_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "KhFJP-1zmhta"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
